#!/usr/bin/env python3
"""
Probability Lookup Table Manager
Handles versioned probability lookup tables with validation and production swapping.

This manager works with tables generated by probability_lookup_generator.py
and provides safe versioning and swapping capabilities for production use.
"""

import os
import sys
import psycopg2
import json
from datetime import datetime
from typing import Dict, Any, Optional, List

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import config if available, otherwise use environment variables
try:
    from backend.core.config.config_manager import config
except ImportError:
    config = None

# PostgreSQL connection parameters
POSTGRES_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': int(os.getenv('POSTGRES_PORT', '5432')),
    'database': os.getenv('POSTGRES_DB', 'rec_io_db'),
    'user': os.getenv('POSTGRES_USER', 'rec_io_user'),
    'password': os.getenv('POSTGRES_PASSWORD', '')
}

class ProbabilityLookupTableManager:
    """Manages versioned probability lookup tables with validation and swapping."""
    
    def __init__(self, symbol: str = "btc"):
        self.symbol = symbol.lower()
        self.db_config = POSTGRES_CONFIG
        self.base_table_name = f"probability_lookup_{self.symbol}"
        self.analytics_schema = "analytics"
        
    def get_versioned_table_name(self, date: Optional[str] = None) -> str:
        """Generate versioned table name with date suffix."""
        if date is None:
            date = datetime.now().strftime("%Y%m%d")
        return f"{self.base_table_name}_{date}"
    
    def create_versioned_table(self, source_table: str, target_date: Optional[str] = None) -> str:
        """
        Create a versioned copy of the source table.
        
        Args:
            source_table: Name of the source table to copy
            target_date: Date suffix for the new table (defaults to today)
            
        Returns:
            Name of the created versioned table
        """
        versioned_name = self.get_versioned_table_name(target_date)
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            
            # Create versioned table as a copy of the source
            create_sql = f"""
            CREATE TABLE {self.analytics_schema}.{versioned_name} AS 
            SELECT * FROM {self.analytics_schema}.{source_table}
            """
            
            cursor.execute(create_sql)
            conn.commit()
            
            print(f"✅ Created versioned table: {self.analytics_schema}.{versioned_name}")
            return versioned_name
            
        except Exception as e:
            print(f"❌ Error creating versioned table: {e}")
            return None
        finally:
            if conn:
                conn.close()
    
    def validate_table_structure(self, table_name: str) -> Dict[str, Any]:
        """
        Validate that a table has the correct structure for probability lookup.
        
        Args:
            table_name: Name of the table to validate
            
        Returns:
            Dictionary with validation results
        """
        validation_result = {
            "valid": False,
            "errors": [],
            "warnings": [],
            "table_info": {}
        }
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            
            # Check if table exists
            cursor.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = '{self.analytics_schema}' 
                    AND table_name = '{table_name}'
                )
            """)
            
            if not cursor.fetchone()[0]:
                validation_result["errors"].append(f"Table {self.analytics_schema}.{table_name} does not exist")
                return validation_result
            
            # Get table structure
            cursor.execute(f"""
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_schema = '{self.analytics_schema}' 
                AND table_name = '{table_name}'
                ORDER BY ordinal_position
            """)
            
            columns = cursor.fetchall()
            validation_result["table_info"]["columns"] = [
                {"name": col[0], "type": col[1], "nullable": col[2]} 
                for col in columns
            ]
            
            # Validate required columns
            required_columns = [
                "ttc_seconds", "buffer_points", "momentum_bucket", 
                "prob_within_positive", "prob_within_negative"
            ]
            
            column_names = [col["name"] for col in validation_result["table_info"]["columns"]]
            
            for required_col in required_columns:
                if required_col not in column_names:
                    validation_result["errors"].append(f"Missing required column: {required_col}")
            
            # Check data types
            for col in validation_result["table_info"]["columns"]:
                if col["name"] == "ttc_seconds" and col["type"] not in ["integer", "bigint"]:
                    validation_result["errors"].append(f"ttc_seconds should be integer, got {col['type']}")
                elif col["name"] == "buffer_points" and col["type"] not in ["integer", "bigint"]:
                    validation_result["errors"].append(f"buffer_points should be integer, got {col['type']}")
                elif col["name"] == "momentum_bucket" and col["type"] not in ["integer", "bigint"]:
                    validation_result["errors"].append(f"momentum_bucket should be integer, got {col['type']}")
                elif col["name"] in ["prob_within_positive", "prob_within_negative"] and col["type"] not in ["numeric", "decimal", "real", "double precision"]:
                    validation_result["errors"].append(f"{col['name']} should be numeric, got {col['type']}")
            
            # Check data ranges and counts
            cursor.execute(f"SELECT COUNT(*) FROM {self.analytics_schema}.{table_name}")
            total_rows = cursor.fetchone()[0]
            validation_result["table_info"]["total_rows"] = total_rows
            
            if total_rows < 1000000:  # Should have at least 1M rows
                validation_result["warnings"].append(f"Table has only {total_rows:,} rows, expected millions")
            
            # Check TTC range
            cursor.execute(f"SELECT MIN(ttc_seconds), MAX(ttc_seconds) FROM {self.analytics_schema}.{table_name}")
            min_ttc, max_ttc = cursor.fetchone()
            validation_result["table_info"]["ttc_range"] = {"min": min_ttc, "max": max_ttc}
            
            if min_ttc != 0 or max_ttc < 3600:
                validation_result["warnings"].append(f"TTC range is {min_ttc}-{max_ttc}, expected 0-3600")
            
            # Check buffer range
            cursor.execute(f"SELECT MIN(buffer_points), MAX(buffer_points) FROM {self.analytics_schema}.{table_name}")
            min_buffer, max_buffer = cursor.fetchone()
            validation_result["table_info"]["buffer_range"] = {"min": min_buffer, "max": max_buffer}
            
            if min_buffer != 0 or max_buffer < 2000:
                validation_result["warnings"].append(f"Buffer range is {min_buffer}-{max_buffer}, expected 0-2000 (BTC-specific)")
            
            # Check momentum bucket range
            cursor.execute(f"SELECT MIN(momentum_bucket), MAX(momentum_bucket) FROM {self.analytics_schema}.{table_name}")
            min_momentum, max_momentum = cursor.fetchone()
            validation_result["table_info"]["momentum_range"] = {"min": min_momentum, "max": max_momentum}
            
            if min_momentum < -30 or max_momentum > 30:
                validation_result["warnings"].append(f"Momentum range is {min_momentum}-{max_momentum}, expected -30 to 30")
            
            # Check probability value ranges
            cursor.execute(f"""
                SELECT 
                    MIN(prob_within_positive), MAX(prob_within_positive),
                    MIN(prob_within_negative), MAX(prob_within_negative)
                FROM {self.analytics_schema}.{table_name}
            """)
            min_pos, max_pos, min_neg, max_neg = cursor.fetchone()
            validation_result["table_info"]["probability_ranges"] = {
                "positive": {"min": float(min_pos), "max": float(max_pos)},
                "negative": {"min": float(min_neg), "max": float(max_neg)}
            }
            
            if min_pos < 0 or max_pos > 100 or min_neg < 0 or max_neg > 100:
                validation_result["errors"].append("Probability values should be between 0 and 100")
            
            # If no errors, mark as valid
            if not validation_result["errors"]:
                validation_result["valid"] = True
                print(f"✅ Table {self.analytics_schema}.{table_name} validation passed")
            else:
                print(f"❌ Table {self.analytics_schema}.{table_name} validation failed")
                for error in validation_result["errors"]:
                    print(f"   Error: {error}")
            
            if validation_result["warnings"]:
                print(f"⚠️  Warnings for {self.analytics_schema}.{table_name}:")
                for warning in validation_result["warnings"]:
                    print(f"   Warning: {warning}")
            
            return validation_result
            
        except Exception as e:
            validation_result["errors"].append(f"Validation error: {e}")
            print(f"❌ Error during validation: {e}")
            return validation_result
        finally:
            if conn:
                conn.close()
    
    def swap_production_table(self, new_table: str, backup_old: bool = True) -> bool:
        """
        Swap the production table with a new validated table.
        
        Args:
            new_table: Name of the new table to make production
            backup_old: Whether to backup the old production table
            
        Returns:
            True if swap was successful
        """
        try:
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            
            # Validate the new table first
            validation = self.validate_table_structure(new_table)
            if not validation["valid"]:
                print("❌ Cannot swap: new table validation failed")
                return False
            
            # Check if current production table exists
            cursor.execute(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = '{self.analytics_schema}' 
                    AND table_name = '{self.base_table_name}'
                )
            """)
            
            current_exists = cursor.fetchone()[0]
            
            if current_exists and backup_old:
                # Create backup of current production table
                backup_name = f"{self.base_table_name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                cursor.execute(f"ALTER TABLE {self.analytics_schema}.{self.base_table_name} RENAME TO {backup_name}")
                print(f"✅ Backed up current production table to: {self.analytics_schema}.{backup_name}")
            
            # Rename new table to production name
            cursor.execute(f"ALTER TABLE {self.analytics_schema}.{new_table} RENAME TO {self.base_table_name}")
            print(f"✅ Swapped {new_table} to production table: {self.analytics_schema}.{self.base_table_name}")
            
            conn.commit()
            return True
            
        except Exception as e:
            print(f"❌ Error during table swap: {e}")
            return False
        finally:
            if conn:
                conn.close()
    
    def list_versioned_tables(self) -> List[str]:
        """List all versioned tables for this symbol."""
        try:
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            
            cursor.execute(f"""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = '{self.analytics_schema}' 
                AND table_name LIKE '{self.base_table_name}_%'
                ORDER BY table_name DESC
            """)
            
            tables = [row[0] for row in cursor.fetchall()]
            return tables
            
        except Exception as e:
            print(f"❌ Error listing versioned tables: {e}")
            return []
        finally:
            if conn:
                conn.close()

def main():
    """Test the table manager functionality."""
    manager = ProbabilityLookupTableManager("btc")
    
    print("🔍 Probability Lookup Table Manager")
    print(f"Symbol: {manager.symbol.upper()}")
    print(f"Base table: {manager.base_table_name}")
    print()
    
    # List existing versioned tables
    versioned_tables = manager.list_versioned_tables()
    print(f"📋 Versioned tables found: {len(versioned_tables)}")
    for table in versioned_tables:
        print(f"   {table}")
    print()
    
    # Validate current production table
    print("🔍 Validating current production table...")
    validation = manager.validate_table_structure(manager.base_table_name)
    
    if validation["valid"]:
        print("✅ Production table is valid")
        print(f"   Total rows: {validation['table_info']['total_rows']:,}")
        print(f"   TTC range: {validation['table_info']['ttc_range']['min']}-{validation['table_info']['ttc_range']['max']}")
        print(f"   Buffer range: {validation['table_info']['buffer_range']['min']}-{validation['table_info']['buffer_range']['max']}")
        print(f"   Momentum range: {validation['table_info']['momentum_range']['min']}-{validation['table_info']['momentum_range']['max']}")
    else:
        print("❌ Production table validation failed")

if __name__ == "__main__":
    main()
