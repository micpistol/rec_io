# System Data Packaging Guide

This guide explains how to package and transfer system-critical data between machines for the rec_io trading platform.

## Overview

The system data packaging scripts allow you to:
- Export all non-user system data from a PostgreSQL database
- Package it into a single compressed archive
- Import it into a new machine's database

This is useful for:
- Setting up new development environments
- Deploying to production servers
- Creating backups of system data
- Sharing system data between team members

## What Gets Packaged

### Included Data (System-Critical)
- **Analytics Schema**: Probability lookup tables, fingerprint tables
- **Historical Data Schema**: BTC and ETH price history
- **Live Data Schema**: Price logs, strike tables, live market data
- **System Schema**: Health status and system monitoring data
- **Work Progress Schema**: TTC progress tracking data
- **Public Schema System Tables**: fills, positions, trades, active_trades

### Excluded Data (User-Specific)
- **Users Schema**: All user-specific data (trade logs, orders, fills, etc.)
- **User Credentials**: Kalshi API credentials and user settings
- **User Preferences**: Individual user configurations

## Scripts

### 1. Package System Data (`package_system_data.py`)

**Purpose**: Export system data from the current database into a compressed archive.

**Usage**:
```bash
cd /path/to/rec_io_20
python scripts/package_system_data.py
```

**Output**:
- Creates a timestamped archive: `system_data_export_YYYYMMDD_HHMMSS.tar.gz`
- Archive contains SQL files for each table organized by schema
- Includes metadata.json with export information

**Example Output**:
```
ğŸš€ System Data Packaging Script
==================================================
ğŸ“ Output directory: system_data_export_20250814_143022

ğŸ”Œ Connecting to database...
  âœ“ Database connection established

ğŸ“Š Exporting system schemas...

ğŸ“¦ Schema: analytics
  Exporting 61 tables from schema 'analytics':
    - btc_fingerprint_directional_baseline
      âœ“ Exported 1234 bytes
    - btc_fingerprint_directional_momentum_-30
      âœ“ Exported 5678 bytes
    ...

ğŸ“¦ Schema: historical_data
  Exporting 2 tables from schema 'historical_data':
    - btc_price_history
      âœ“ Exported 1234567 bytes
    - eth_price_history
      âœ“ Exported 987654 bytes

...

ğŸ—œï¸ Creating compressed archive...
  âœ“ Created archive: system_data_export_20250814_143022.tar.gz
  ğŸ“ Archive size: 2.34 MB

âœ… System data packaging completed successfully!
ğŸ“¦ Archive: system_data_export_20250814_143022.tar.gz
ğŸ“ Size: 2.34 MB

ğŸ“‹ Next steps:
   1. Copy system_data_export_20250814_143022.tar.gz to target machine
   2. Run unpack_system_data.py on target machine
```

### 2. Unpack System Data (`unpack_system_data.py`)

**Purpose**: Import system data from an archive into a new database.

**Usage**:
```bash
cd /path/to/rec_io_20
python scripts/unpack_system_data.py system_data_export_20250814_143022.tar.gz
```

**Options**:
- `--force`: Force import even if data already exists (skips confirmation prompts)

**Behavior**:
- If no tables exist with data, the script will automatically import all data
- If tables exist with data, the script will prompt for confirmation before overwriting
- Use `--force` to skip the confirmation prompt and automatically overwrite existing data

**Example Output**:
```
ğŸš€ System Data Unpacking Script
==================================================
ğŸ“¦ Extracting archive: system_data_export_20250814_143022.tar.gz
  âœ“ Extracted to: temp_extract_20250814_143045/system_data_export_20250814_143022

ğŸ“‹ Loaded metadata:
  Export timestamp: 2025-08-14T14:30:22.123456
  Version: 1.0
  Schemas: analytics, historical_data, live_data, system, work_progress
  Public tables: fills, positions, trades, active_trades

ğŸ”Œ Connecting to database...
  âœ“ Database connection established

ğŸ” Checking existing data in database...
   ğŸ“Š analytics.btc_fingerprint_directional_baseline: 1,234 rows
   ğŸ“Š historical_data.btc_price_history: 123,456 rows

âš ï¸  Found 2 tables with existing data:
   - analytics.btc_fingerprint_directional_baseline: 1,234 rows
   - historical_data.btc_price_history: 123,456 rows

â“ Do you want to overwrite existing data? (y/N): y

ğŸ—ï¸ Creating schemas if they don't exist...
  âœ“ Schema 'analytics' ready
  âœ“ Schema 'historical_data' ready
  âœ“ Schema 'live_data' ready
  âœ“ Schema 'system' ready
  âœ“ Schema 'work_progress' ready

ğŸ“¥ Importing schema data...

ğŸ“¦ Schema: analytics
  ğŸ“¥ Importing 61 tables to schema 'analytics':
    âœ“ Imported btc_fingerprint_directional_baseline
    âœ“ Imported btc_fingerprint_directional_momentum_-30
    ...

ğŸ“¦ Schema: historical_data
  ğŸ“¥ Importing 2 tables to schema 'historical_data':
    âœ“ Imported btc_price_history
    âœ“ Imported eth_price_history

...

ğŸ” Verifying import...
  ğŸ“Š Schema 'analytics': 61 tables
    - btc_fingerprint_directional_baseline: 1,234 rows
    - btc_fingerprint_directional_momentum_-30: 5,678 rows
    ...
  ğŸ“Š Schema 'historical_data': 2 tables
    - btc_price_history: 123,456 rows
    - eth_price_history: 98,765 rows
  ...

ğŸ“ˆ Import Summary:
  Total tables: 163
  Total rows: 2,345,678

âœ… System data import completed successfully!
ğŸ“‹ Next steps:
   1. Verify the application starts correctly
   2. Check that all system functionality works
   3. Set up user accounts and credentials

ğŸ§¹ Cleaned up temporary files
```

## Prerequisites

### Source Machine (Packaging)
- PostgreSQL database with rec_io_db
- Python environment with required dependencies
- Access to database with rec_io_user credentials
- pg_dump command available

### Target Machine (Unpacking)
- PostgreSQL server running
- rec_io_db database created
- Python environment with required dependencies
- Access to database with rec_io_user credentials
- psql command available

## Database Configuration

Both scripts use the ConfigManager to get database connection details from:
- `backend/core/config/config.default.json`
- `backend/core/config/config.local.json`

Ensure these files are properly configured on both machines.

## Archive Structure

The packaged archive contains:
```
system_data_export_YYYYMMDD_HHMMSS/
â”œâ”€â”€ metadata.json                    # Export metadata and instructions
â”œâ”€â”€ analytics/                       # Analytics schema tables
â”‚   â”œâ”€â”€ btc_fingerprint_directional_baseline.sql
â”‚   â”œâ”€â”€ btc_fingerprint_directional_momentum_-30.sql
â”‚   â””â”€â”€ ...
â”œâ”€â”€ historical_data/                 # Historical data schema tables
â”‚   â”œâ”€â”€ btc_price_history.sql
â”‚   â””â”€â”€ eth_price_history.sql
â”œâ”€â”€ live_data/                       # Live data schema tables
â”‚   â”œâ”€â”€ btc_price_log.sql
â”‚   â”œâ”€â”€ btc_strike_table.sql
â”‚   â””â”€â”€ ...
â”œâ”€â”€ system/                          # System schema tables
â”‚   â””â”€â”€ health_status.sql
â”œâ”€â”€ work_progress/                   # Work progress schema tables
â”‚   â”œâ”€â”€ ttc_0069_btc.sql
â”‚   â”œâ”€â”€ ttc_0070_btc.sql
â”‚   â””â”€â”€ ...
â””â”€â”€ public/                          # Public schema system tables
    â”œâ”€â”€ fills.sql
    â”œâ”€â”€ positions.sql
    â”œâ”€â”€ trades.sql
    â””â”€â”€ active_trades.sql
```

## Troubleshooting

### Common Issues

1. **Database Connection Failed**
   - Verify database is running
   - Check credentials in config files
   - Ensure network connectivity

2. **Permission Denied**
   - Check file permissions on scripts
   - Ensure database user has appropriate privileges
   - Verify pg_dump/psql access

3. **Import Errors**
   - Check if tables already exist (use --force if needed)
   - Verify schema creation permissions
   - Review error messages for specific table issues

4. **Archive Corruption**
   - Verify file transfer completed successfully
   - Check file size matches source
   - Re-run packaging if needed

### Verification Steps

After unpacking, verify the import by:
1. Checking table counts in each schema
2. Running a few sample queries
3. Starting the application and testing functionality
4. Verifying system health endpoints

## Security Considerations

- Archive files contain sensitive system data
- Store archives securely
- Use secure transfer methods (SCP, SFTP, etc.)
- Consider encrypting archives for sensitive environments
- Clean up temporary files after import

## Performance Notes

- Large databases may take significant time to package/unpack
- Monitor disk space during extraction
- Consider running during low-usage periods
- Archive size typically 2-3GB for full system data

## Version Compatibility

- Scripts are versioned and include metadata
- Check version compatibility between source and target
- Update scripts if database schema changes significantly
- Test with sample data before production use
